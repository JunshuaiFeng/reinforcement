{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_algorithms_methods_overview\n",
    "\n",
    "Task: List all algorithms and methods that we have covered in this course. Write 3 sentences to describe what each algorithm and method solves etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Dijkstra’s algorithm** is very similar to Prim’s algorithm for minimum spanning tree. We maintain two sets, one set contains vertices included in shortest path tree, other set includes vertices not yet included in shortest path tree. At every step of the algorithm, we find a vertex which is in the other set and has a minimum distance from the source.\n",
    "\n",
    "\n",
    "* **A* Search:** Like Dijkstra, A&ast; works by making a lowest-cost path tree from the start node to the target node. What makes A&ast; different and better for many searches is that for each node, A&ast; uses a function  that gives an estimate of the total cost of a path using that node.\n",
    "\n",
    "\n",
    "* **Minimax Algorithm** is a state-space search tree that represents games where players alternate turns. It computes each node’s minimax value by minimizing the adversarial score and maximizing the agent score. It’s the best achievable utility against a rational (optimal) adversary.\n",
    "\n",
    "\n",
    "* **Alpha-Beta Pruning method** is an optimization technique for minimax algorithm. It reduces the computation time by a huge factor and allows us to search much faster and even go into deeper levels in the game tree. It cuts off branches in the game tree which need not be searched with alpha and beta variables which represent the best value of the maximizer and the minimizer at each level.\n",
    "\n",
    "\n",
    "* **Expectimax search** is used when we don’t know what the result of an action. It computes the average score under optimal play. Since the chance nodes are unknown, we need to take average of value of children to calculate the expected utilities. Principle of maximum expected utility is that an agent should choose the action which maximizes its expected utility given its knowledge.\n",
    "\n",
    "\n",
    "* **Markov Decision Process (MDP) model** contains: a set of states, actions, reward function, and Transition of the each action’s effect in each state. At each time step, the process is in some state s, and the decision maker may choose any action a that is available in state s. The process responds at the next time step by randomly moving into a new state s', and giving the decision maker a corresponding reward. The next state s' depends on the current state s and the decision maker's action a.\n",
    "\n",
    "\n",
    "* MDP can compute an optimal policy that maximizes expected utility with either **Value iteration** or **Policy iteration**. In **Value iteration**, every iteration updates both the values and (implicitly) the policy. We don’t track the policy, but taking the max over actions implicitly recomputes it. **Policy Iteration** includes Policy evaluation and Policy improvement; it first calculates utilities for some fixed policy until convergence and then updates policy using one-step look-ahead with resulting converged utilities as future values. \n",
    "\n",
    "\n",
    "* **Reinforcement Learning** teaches agents ought to take actions in an environment so as to maximize some notion of cumulative reward. All learning is based on observed samples of outcomes and the agent’s utility is defined by the reward function. RL methods can be classified according to: Model-free or model-based; Value-based or policy-based; On-policy or off-policy.\n",
    "\n",
    "\n",
    "* In **Q-learning**, an agent tries to learn the optimal policy from its history of interaction with the environment. It is an off-policy method which is why it learns an optimal policy no matter which policy it is carrying out. Q value can be computed with the formula:<img src=\"q.png\" width=\"450\">\n",
    "\n",
    "\n",
    "* **Approximate Q-Learning** improves Q-Learning by adjusting weights of active features. Here, Q value can be defined as the sum of the feature multiply by its weight. Adjust weights of active features to best optimize the outcomes.\n",
    "<img src=\"q2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
